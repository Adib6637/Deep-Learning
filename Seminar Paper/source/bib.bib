
@article{sancheti_camera_nodate,
	title = {Camera based driver monitoring system using deep learning},
	abstract = {Driver Monitoring is emerging as an essential requirement for Advanced Driving Assistance and Autonomous Driving systems. In this paper we propose a real-time, {IR} camerabased driver monitoring system. Basic driver monitoring features include head tracking, gaze tracking, eye state analysis – blink rate, blink duration, eye open/close all of which can be used to implement driver safety applications like driver distraction and driver drowsiness detection. We propose a system where all these modules have been developed using deep learning which has made the solution more robust to different ethnicities, gender, lighting conditions and occlusions. We have also optimized the solution to run on any embedded platform ({ARM}, {DSP}, {ASICs} etc) without the need of {GPU} or cloud support during runtime. This helps in lowering power consumption and cost making the solution amenable for use in automotive. An implementation of this system is available as part of the See ‘n Sense {DMS} solution from {AllGo}.},
	pages = {5},
	author = {Sancheti, Nirmal Kumar and Srikant, Manjari and Gopal, Krupa H},
	langid = {english},
	annotation = {camera base, block method
face{\textgreater} pose{\textgreater}blink{\textgreater}drowsiness

},
	file = {Sancheti et al. - Camera based driver monitoring system using deep l.pdf:C\:\\Users\\sheik\\Zotero\\storage\\9CFIM96R\\Sancheti et al. - Camera based driver monitoring system using deep l.pdf:application/pdf},
}

@article{phan_efficient_2021,
	title = {An Efficient Approach for Detecting Driver Drowsiness Based on Deep Learning},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/18/8441},
	doi = {10.3390/app11188441},
	abstract = {Drowsy driving is one of the common causes of road accidents resulting in injuries, even death, and significant economic losses to drivers, road users, families, and society. There have been many studies carried out in an attempt to detect drowsiness for alert systems. However, a majority of the studies focused on determining eyelid and mouth movements, which have revealed many limitations for drowsiness detection. Besides, physiological measures-based studies may not be feasible in practice because the measuring devices are often not available on vehicles and often uncomfortable for drivers. In this research, we therefore propose two efficient methods with three scenarios for doze alert systems. The former applies facial landmarks to detect blinks and yawns based on appropriate thresholds for each driver. The latter uses deep learning techniques with two adaptive deep neural networks based on {MobileNet}-V2 and {ResNet}-50V2. The second method analyzes the videos and detects driver’s activities in every frame to learn all features automatically. We leverage the advantage of the transfer learning technique to train the proposed networks on our training dataset. This solves the problem of limited training datasets, provides fast training time, and keeps the advantage of the deep neural networks. Experiments were conducted to test the effectiveness of our methods compared with other methods. Empirical results demonstrate that the proposed method using deep learning techniques can achieve a high accuracy of 97\%. This study provides meaningful solutions in practice to prevent unfortunate automobile accidents caused by drowsiness.},
	pages = {8441},
	number = {18},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Phan, Anh-Cang and Nguyen, Ngoc-Hoang-Quyen and Trieu, Thanh-Ngoan and Phan, Thuong-Cang},
	urldate = {2022-10-17},
	date = {2021-09-11},
	langid = {english},
	file = {Full Text:C\:\\Users\\sheik\\Zotero\\storage\\6H9RIK23\\Phan et al. - 2021 - An Efficient Approach for Detecting Driver Drowsin.pdf:application/pdf},
}

@article{krishna_vision_2022,
	title = {Vision Transformers and {YoloV}5 based Driver Drowsiness Detection Framework},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2209.01401},
	doi = {10.48550/ARXIV.2209.01401},
	abstract = {Human drivers have distinct driving techniques, knowledge, and sentiments due to unique driving traits. Driver drowsiness has been a serious issue endangering road safety; therefore, it is essential to design an effective drowsiness detection algorithm to bypass road accidents. Miscellaneous research efforts have been approached the problem of detecting anomalous human driver behaviour to examine the frontal face of the driver and automobile dynamics via computer vision techniques. Still, the conventional methods cannot capture complicated driver behaviour features. However, with the origin of deep learning architectures, a substantial amount of research has also been executed to analyze and recognize driver's drowsiness using neural network algorithms. This paper introduces a novel framework based on vision transformers and {YoloV}5 architectures for driver drowsiness recognition. A custom {YoloV}5 pre-trained architecture is proposed for face extraction with the aim of extracting Region of Interest ({ROI}). Owing to the limitations of previous architectures, this paper introduces vision transformers for binary image classification which is trained and validated on a public dataset {UTA}-{RLDD}. The model had achieved 96.2{\textbackslash}\% and 97.4{\textbackslash}\% as it's training and validation accuracies respectively. For the further evaluation, proposed framework is tested on a custom dataset of 39 participants in various light circumstances and achieved 95.5{\textbackslash}\% accuracy. The conducted experimentations revealed the significant potential of our framework for practical applications in smart transportation systems.},
	author = {Krishna, Ghanta Sai and Supriya, Kundrapu and Vardhan, Jai and K, Mallikharjuna Rao},
	urldate = {2022-10-17},
	date = {2022},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	annotation = {yolov5, face-based, three method to detect

},
}

@book{hagan_neural_2014,
	location = {s.L},
	edition = {2nd edition},
	title = {Neural network design},
	isbn = {978-0-9717321-1-7},
	pagetotal = {1},
	publisher = {Martin T. Hagan},
	author = {Hagan, Martin T. and Demuth, Howard B. and Beale, Mark Hudson and De Jésus, Orlando},
	date = {2014},
	annotation = {concept of nn
},
}

@book{sewak_practical_2018,
	title = {Practical Convolutional Neural Networks: Implement advanced deep learning models using Python},
	isbn = {978-1-78839-230-3},
	shorttitle = {Practical Convolutional Neural Networks},
	pagetotal = {218},
	publisher = {Packt Publishing},
	author = {Sewak, Mohit and Karim, Md Rezaul and Pujari, Pradeep},
	date = {2018-02-27},
}

@inproceedings{simic_driver_2016,
	location = {Belgrade, Serbia},
	title = {Driver monitoring algorithm for advanced driver assistance systems},
	isbn = {978-1-5090-4086-5},
	url = {http://ieeexplore.ieee.org/document/7818908/},
	doi = {10.1109/TELFOR.2016.7818908},
	eventtitle = {2016 24th Telecommunications Forum ({TELFOR})},
	pages = {1--4},
	booktitle = {2016 24th Telecommunications Forum ({TELFOR})},
	publisher = {{IEEE}},
	author = {Simic, Aleksandra and Kocic, Ognjen and Bjelica, Milan Z. and Milosevic, Milena},
	urldate = {2022-10-17},
	date = {2016-11},
	annotation = {{ADAS} is constantly providing new solutions

face and eye
},
}

@article{yue_practical_2020,
	title = {The Practical Effectiveness of Advanced Driver Assistance Systems at Different Roadway Facilities: System Limitation, Adoption, and Usage},
	volume = {21},
	issn = {1524-9050, 1558-0016},
	url = {https://ieeexplore.ieee.org/document/8809914/},
	doi = {10.1109/TITS.2019.2935195},
	shorttitle = {The Practical Effectiveness of Advanced Driver Assistance Systems at Different Roadway Facilities},
	pages = {3859--3870},
	number = {9},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	shortjournal = {{IEEE} Trans. Intell. Transport. Syst.},
	author = {Yue, Lishengsa and Abdel-Aty, Mohamed A. and Wu, Yina and Farid, Ahmed},
	urldate = {2022-10-17},
	date = {2020-09},
	annotation = {{ADAS} can reduce crash
},
}

@online{matine_what_2021,
	title = {What Percentage of Car Accidents Are Caused by Human Error? {\textbar} Pittsburgh Law Blog},
	url = {https://www.cbmclaw.com/what-percentage-of-car-accidents-are-caused-by-human-error/},
	shorttitle = {What Percentage of Car Accidents Are Caused by Human Error?},
	abstract = {What Percentage of Car Accidents Are Caused by Human Error? - Visit our Pittsburgh law blog to learn more today!},
	titleaddon = {Caroselli, Beachler \& Coleman, L.L.C.},
	author = {Matine, David},
	urldate = {2022-10-17},
	date = {2021-09-22},
	langid = {american},
	annotation = {percentage of car accident due to human error
},
	file = {Snapshot:C\:\\Users\\sheik\\Zotero\\storage\\4EY5IMGB\\what-percentage-of-car-accidents-are-caused-by-human-error.html:text/html},
}

@online{tsang_reading_2020,
	title = {Reading: C3 — Concentrated-Comprehensive Convolution (Semantic Segmentation)},
	url = {https://sh-tsang.medium.com/reading-c3-concentrated-comprehensive-convolution-semantic-segmentation-5b6dd3fb46b2},
	shorttitle = {Reading},
	abstract = {Compared to {ESPNet}, {ERFNet}, {DRN} \& {ENet}, Similar or Improved {mIOU} Achieved While Obtaining Smaller model sizes and fewer number of {FLOPs}},
	titleaddon = {Medium},
	author = {Tsang, Sik-Ho},
	urldate = {2022-10-19},
	date = {2020-10-11},
	langid = {english},
	annotation = {c3 improve performance reduce parameter by half
},
	file = {Snapshot:C\:\\Users\\sheik\\Zotero\\storage\\Y2G4TSCM\\reading-c3-concentrated-comprehensive-convolution-semantic-segmentation-5b6dd3fb46b2.html:text/html},
}

@article{mahdianpari_very_2018,
	title = {Very Deep Convolutional Neural Networks for Complex Land Cover Mapping Using Multispectral Remote Sensing Imagery},
	volume = {10},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/10/7/1119},
	doi = {10.3390/rs10071119},
	abstract = {Despite recent advances of deep Convolutional Neural Networks ({CNNs}) in various computer vision tasks, their potential for classification of multispectral remote sensing images has not been thoroughly explored. In particular, the applications of deep {CNNs} using optical remote sensing data have focused on the classification of very high-resolution aerial and satellite data, owing to the similarity of these data to the large datasets in computer vision. Accordingly, this study presents a detailed investigation of state-of-the-art deep learning tools for classification of complex wetland classes using multispectral {RapidEye} optical imagery. Specifically, we examine the capacity of seven well-known deep convnets, namely {DenseNet}121, {InceptionV}3, {VGG}16, {VGG}19, Xception, {ResNet}50, and {InceptionResNetV}2, for wetland mapping in Canada. In addition, the classification results obtained from deep {CNNs} are compared with those based on conventional machine learning tools, including Random Forest and Support Vector Machine, to further evaluate the efficiency of the former to classify wetlands. The results illustrate that the full-training of convnets using five spectral bands outperforms the other strategies for all convnets. {InceptionResNetV}2, {ResNet}50, and Xception are distinguished as the top three convnets, providing state-of-the-art classification accuracies of 96.17\%, 94.81\%, and 93.57\%, respectively. The classification accuracies obtained using Support Vector Machine ({SVM}) and Random Forest ({RF}) are 74.89\% and 76.08\%, respectively, considerably inferior relative to {CNNs}. Importantly, {InceptionResNetV}2 is consistently found to be superior compared to all other convnets, suggesting the integration of Inception and {ResNet} modules is an efficient architecture for classifying complex remote sensing scenes such as wetlands.},
	pages = {1119},
	number = {7},
	journaltitle = {Remote Sensing},
	shortjournal = {Remote Sensing},
	author = {Mahdianpari, Masoud and Salehi, Bahram and Rezaee, Mohammad and Mohammadimanesh, Fariba and Zhang, Yun},
	urldate = {2022-10-21},
	date = {2018-07-14},
	langid = {english},
	annotation = {inceptionv3
},
	file = {Full Text:C\:\\Users\\sheik\\Zotero\\storage\\JHXDP7EW\\Mahdianpari et al. - 2018 - Very Deep Convolutional Neural Networks for Comple.pdf:application/pdf},
}

@online{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover {TensorFlow}'s flexible ecosystem of tools, libraries and community resources.},
	titleaddon = {{TensorFlow}},
	urldate = {2022-10-21},
	langid = {english},
	annotation = {tensorflow
},
	file = {Snapshot:C\:\\Users\\sheik\\Zotero\\storage\\ARHFEJ45\\www.tensorflow.org.html:text/html},
}

@online{gaillard_epoch_nodate,
	title = {Epoch (machine learning) {\textbar} Radiology Reference Article {\textbar} Radiopaedia.org},
	url = {https://radiopaedia.org/articles/epoch-machine-learning},
	abstract = {An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very larg...},
	titleaddon = {Radiopaedia},
	author = {Gaillard, Frank},
	urldate = {2022-10-22},
	langid = {british},
	doi = {10.53347/rID-56141},
	annotation = {epoch
},
	file = {Snapshot:C\:\\Users\\sheik\\Zotero\\storage\\CP5FZA5N\\epoch-machine-learning.html:text/html},
}

@online{brownlee_what_2019,
	title = {What is Deep Learning?},
	url = {https://machinelearningmastery.com/what-is-deep-learning/},
	abstract = {Deep Learning {\textbar} Interested in learning more about deep learning and artificial neural networks? Discover exactly what deep learning is by hearing from a range of experts and leaders in the field.},
	titleaddon = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	urldate = {2022-10-24},
	date = {2019-08-15},
	langid = {american},
	annotation = {very large neural networks we can now have and … huge amounts of data that we have access to
},
	file = {Snapshot:C\:\\Users\\sheik\\Zotero\\storage\\XQY2RZYX\\what-is-deep-learning.html:text/html},
}

@inproceedings{dongmei_classification_2020,
	title = {Classification and identification of citrus pests based on {InceptionV}3 convolutional neural network and migration learning},
	doi = {10.1109/ITIA50152.2020.9312359},
	abstract = {As one of the origins of citrus in the world, China has a large number of excellent citrus resources and mature cultivation techniques. Pests and diseases have become an important constraint on citrus harvest and quality. At present, deep learning has been widely used in many fields, and its application in agricultural research is gradually becoming mature. The use of deep learning convolutional neural networks to identify citrus pests is an effective and high-discrimination recognition technology. In this paper, based on a small amount of self-collected citrus pests dataset, including Blowing scale, Moth, Starscream, Star beetle, Citrus fruit fly, a total of 5 common pests and diseases, and propose a combination of Inceptionv3 network feature extraction model and migration learning According to the classification and recognition method, the final recognition accuracy can reach 96.81\%.},
	eventtitle = {2020 International Conference on Internet of Things and Intelligent Applications ({ITIA})},
	pages = {1--7},
	booktitle = {2020 International Conference on Internet of Things and Intelligent Applications ({ITIA})},
	author = {Dongmei, Zhou and Ke, Wang and Hongbo, Guo and Peng, Wang and Chao, Wang and Shaofeng, Peng},
	date = {2020-11},
	keywords = {citrus pests and diseases, Computational modeling, Convolution, Data models, deep learning, Deep learning, Diseases, Feature extraction, Inceptionv3, migration learning, Training},
	annotation = {{inceptionV}3
},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sheik\\Zotero\\storage\\RQXFG6XZ\\9312359.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sheik\\Zotero\\storage\\6CK2PQH6\\Dongmei et al. - 2020 - Classification and identification of citrus pests .pdf:application/pdf},
}
